{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7652\\4182027630.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Call the function with the feature you want to plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mplot_time_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'feature_1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7652\\4182027630.py\u001b[0m in \u001b[0;36mplot_time_series\u001b[1;34m(df, feature)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Function to plot time series for each portfolio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot_time_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mportfolios\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Portfolio'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Descriptive Statistics\n",
    "print(\"Training Data Descriptive Statistics:\")\n",
    "print(X_agg_train.describe())\n",
    "\n",
    "print(\"\\nTesting Data Descriptive Statistics:\")\n",
    "print(X_agg_test.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SpecificationError",
     "evalue": "nested renamer is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSpecificationError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16228\\1729067533.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m# Example usage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# Assume X_train and X_test are already loaded DataFrames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0maggregated_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maggregate_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[0maggregated_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maggregate_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16228\\1729067533.py\u001b[0m in \u001b[0;36maggregate_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# Apply aggregations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0maggregated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Portfolio'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maggregations_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# Flatten the MultiIndex columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\decroux paul\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGroupByApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 869\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\decroux paul\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36magg\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[1;31m# we require a list, but not a 'str'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\decroux paul\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36magg_dict_like\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    471\u001b[0m             \u001b[0mselection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m         \u001b[0marg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_dictlike_arg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"agg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mselected_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\decroux paul\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mnormalize_dictlike_arg\u001b[1;34m(self, how, obj, func)\u001b[0m\n\u001b[0;32m    582\u001b[0m         ) or (any(is_dict_like(v) for _, v in func.items())):\n\u001b[0;32m    583\u001b[0m             \u001b[1;31m# GH 15931 - deprecation of renaming keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mSpecificationError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nested renamer is not supported\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSpecificationError\u001b[0m: nested renamer is not supported"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Correlation Matrix\n",
    "corr_train = X_agg_train.corr()\n",
    "corr_test = X_agg_test.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_train, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title('Correlation Matrix for Training Data')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_test, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title('Correlation Matrix for Testing Data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of features\n",
    "for column in X_agg_train.columns:\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    sns.histplot(X_agg_train[column], kde=True, color='blue', label='Train')\n",
    "    sns.histplot(X_agg_test[column], kde=True, color='red', label='Test', alpha=0.6)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data analysis\n",
    "print(\"Missing Data in Training Dataset:\")\n",
    "print(X_agg_train.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing Data in Testing Dataset:\")\n",
    "print(X_agg_test.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def train_test_split_adapted(engine):\n",
    "    # Ensure 'Date' is in datetime format\n",
    "    engine.df_concatenated['Date'] = pd.to_datetime(engine.df_concatenated['Date'])\n",
    "    \n",
    "    # Sort data by date to ensure chronological order\n",
    "    engine.df_concatenated.sort_values('Date', inplace=True)\n",
    "    \n",
    "    # Determine the cutoff date at two-thirds of the dataset\n",
    "    unique_dates = engine.df_concatenated['Date'].unique()\n",
    "    cutoff_index = int(len(unique_dates) * 0.67)\n",
    "    cutoff_date = unique_dates[cutoff_index]\n",
    "\n",
    "    # Split the data based on the cutoff date\n",
    "    engine.X_train = engine.df_concatenated[engine.df_concatenated['Date'] <= cutoff_date].drop(columns=['Type'])\n",
    "    engine.X_test = engine.df_concatenated[engine.df_concatenated['Date'] > cutoff_date].drop(columns=['Type'])\n",
    "    engine.y_train = engine.df_concatenated[engine.df_concatenated['Date'] <= cutoff_date]['Type']\n",
    "    engine.y_test = engine.df_concatenated[engine.df_concatenated['Date'] > cutoff_date]['Type']\n",
    "    \n",
    "    # Reset index for both training and testing sets\n",
    "    engine.X_train.reset_index(drop=True, inplace=True)\n",
    "    engine.X_test.reset_index(drop=True, inplace=True)\n",
    "    engine.y_train.reset_index(drop=True, inplace=True)\n",
    "    engine.y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return engine.X_train, engine.X_test, engine.y_train, engine.y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming X_train and X_test are already loaded as pandas DataFrames\n",
    "\n",
    "# Calculate the mean of each column in the training data\n",
    "train_means = X_train.mean()\n",
    "\n",
    "# Fill missing values in X_train with the mean of each column\n",
    "X_train = X_train.fillna(train_means)\n",
    "\n",
    "# It's often a good practice to fill missing values in the test set\n",
    "# using means calculated from the training set to avoid data leakage\n",
    "X_test = X_test.fillna(train_means)\n",
    "\n",
    "# Optionally, you can print the datasets to verify that the missing values have been filled\n",
    "print(X_train.head())\n",
    "print(X_test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-28 17:44:14,549] A new study created in memory with name: no-name-546e5b1f-4049-4c9e-9260-557dd7ab547c\n",
      "[I 2024-08-28 17:44:16,830] Trial 0 finished with value: 0.884 and parameters: {'n_estimators': 238, 'max_depth': 31, 'min_samples_split': 12, 'max_features': 'log2'}. Best is trial 0 with value: 0.884.\n",
      "c:\\Users\\decroux paul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "[I 2024-08-28 17:44:18,812] Trial 1 finished with value: 0.892 and parameters: {'n_estimators': 292, 'max_depth': 7, 'min_samples_split': 8, 'max_features': 'auto'}. Best is trial 1 with value: 0.892.\n",
      "c:\\Users\\decroux paul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "[I 2024-08-28 17:44:19,926] Trial 2 finished with value: 0.892 and parameters: {'n_estimators': 231, 'max_depth': 10, 'min_samples_split': 3, 'max_features': 'auto'}. Best is trial 1 with value: 0.892.\n",
      "[I 2024-08-28 17:44:20,297] Trial 3 finished with value: 0.892 and parameters: {'n_estimators': 99, 'max_depth': 7, 'min_samples_split': 4, 'max_features': 'log2'}. Best is trial 1 with value: 0.892.\n",
      "[I 2024-08-28 17:44:21,161] Trial 4 finished with value: 0.884 and parameters: {'n_estimators': 202, 'max_depth': 10, 'min_samples_split': 19, 'max_features': 'log2'}. Best is trial 1 with value: 0.892.\n",
      "[I 2024-08-28 17:44:21,519] Trial 5 finished with value: 0.896 and parameters: {'n_estimators': 82, 'max_depth': 11, 'min_samples_split': 10, 'max_features': 'sqrt'}. Best is trial 5 with value: 0.896.\n",
      "c:\\Users\\decroux paul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "[I 2024-08-28 17:44:21,710] Trial 6 finished with value: 0.892 and parameters: {'n_estimators': 57, 'max_depth': 4, 'min_samples_split': 20, 'max_features': 'auto'}. Best is trial 5 with value: 0.896.\n",
      "[I 2024-08-28 17:44:22,132] Trial 7 finished with value: 0.9 and parameters: {'n_estimators': 87, 'max_depth': 28, 'min_samples_split': 6, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:22,798] Trial 8 finished with value: 0.884 and parameters: {'n_estimators': 165, 'max_depth': 7, 'min_samples_split': 15, 'max_features': 'log2'}. Best is trial 7 with value: 0.9.\n",
      "c:\\Users\\decroux paul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "[I 2024-08-28 17:44:23,659] Trial 9 finished with value: 0.888 and parameters: {'n_estimators': 205, 'max_depth': 9, 'min_samples_split': 7, 'max_features': 'auto'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:24,199] Trial 10 finished with value: 0.892 and parameters: {'n_estimators': 127, 'max_depth': 28, 'min_samples_split': 2, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:24,408] Trial 11 finished with value: 0.896 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 9, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:24,899] Trial 12 finished with value: 0.888 and parameters: {'n_estimators': 110, 'max_depth': 18, 'min_samples_split': 12, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:25,256] Trial 13 finished with value: 0.896 and parameters: {'n_estimators': 85, 'max_depth': 23, 'min_samples_split': 6, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:25,843] Trial 14 finished with value: 0.896 and parameters: {'n_estimators': 147, 'max_depth': 15, 'min_samples_split': 10, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:26,183] Trial 15 finished with value: 0.892 and parameters: {'n_estimators': 82, 'max_depth': 25, 'min_samples_split': 14, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:26,812] Trial 16 finished with value: 0.892 and parameters: {'n_estimators': 132, 'max_depth': 15, 'min_samples_split': 6, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:26,984] Trial 17 finished with value: 0.884 and parameters: {'n_estimators': 73, 'max_depth': 2, 'min_samples_split': 16, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:27,521] Trial 18 finished with value: 0.896 and parameters: {'n_estimators': 111, 'max_depth': 32, 'min_samples_split': 5, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:28,298] Trial 19 finished with value: 0.888 and parameters: {'n_estimators': 158, 'max_depth': 13, 'min_samples_split': 11, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:29,321] Trial 20 finished with value: 0.896 and parameters: {'n_estimators': 198, 'max_depth': 21, 'min_samples_split': 8, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:29,670] Trial 21 finished with value: 0.884 and parameters: {'n_estimators': 60, 'max_depth': 20, 'min_samples_split': 9, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:29,971] Trial 22 finished with value: 0.896 and parameters: {'n_estimators': 53, 'max_depth': 27, 'min_samples_split': 10, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:30,453] Trial 23 finished with value: 0.888 and parameters: {'n_estimators': 91, 'max_depth': 18, 'min_samples_split': 13, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:30,777] Trial 24 finished with value: 0.892 and parameters: {'n_estimators': 72, 'max_depth': 29, 'min_samples_split': 8, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:31,373] Trial 25 finished with value: 0.892 and parameters: {'n_estimators': 129, 'max_depth': 24, 'min_samples_split': 5, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:31,667] Trial 26 finished with value: 0.892 and parameters: {'n_estimators': 52, 'max_depth': 14, 'min_samples_split': 10, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:32,240] Trial 27 finished with value: 0.884 and parameters: {'n_estimators': 109, 'max_depth': 20, 'min_samples_split': 7, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:32,668] Trial 28 finished with value: 0.9 and parameters: {'n_estimators': 74, 'max_depth': 26, 'min_samples_split': 16, 'max_features': 'log2'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:33,964] Trial 29 finished with value: 0.888 and parameters: {'n_estimators': 275, 'max_depth': 30, 'min_samples_split': 18, 'max_features': 'log2'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:35,423] Trial 30 finished with value: 0.888 and parameters: {'n_estimators': 177, 'max_depth': 26, 'min_samples_split': 16, 'max_features': 'log2'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:36,185] Trial 31 finished with value: 0.888 and parameters: {'n_estimators': 78, 'max_depth': 22, 'min_samples_split': 13, 'max_features': 'log2'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:36,874] Trial 32 finished with value: 0.896 and parameters: {'n_estimators': 68, 'max_depth': 31, 'min_samples_split': 11, 'max_features': 'log2'}. Best is trial 7 with value: 0.9.\n",
      "c:\\Users\\decroux paul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "[I 2024-08-28 17:44:38,020] Trial 33 finished with value: 0.892 and parameters: {'n_estimators': 102, 'max_depth': 26, 'min_samples_split': 9, 'max_features': 'auto'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:38,961] Trial 34 finished with value: 0.888 and parameters: {'n_estimators': 97, 'max_depth': 17, 'min_samples_split': 12, 'max_features': 'log2'}. Best is trial 7 with value: 0.9.\n",
      "c:\\Users\\decroux paul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "[I 2024-08-28 17:44:40,105] Trial 35 finished with value: 0.888 and parameters: {'n_estimators': 117, 'max_depth': 12, 'min_samples_split': 17, 'max_features': 'auto'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:40,785] Trial 36 finished with value: 0.9 and parameters: {'n_estimators': 67, 'max_depth': 28, 'min_samples_split': 4, 'max_features': 'log2'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:43,385] Trial 37 finished with value: 0.892 and parameters: {'n_estimators': 262, 'max_depth': 30, 'min_samples_split': 4, 'max_features': 'log2'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:44,255] Trial 38 finished with value: 0.896 and parameters: {'n_estimators': 86, 'max_depth': 28, 'min_samples_split': 2, 'max_features': 'log2'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:45,787] Trial 39 finished with value: 0.892 and parameters: {'n_estimators': 149, 'max_depth': 32, 'min_samples_split': 3, 'max_features': 'log2'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:47,914] Trial 40 finished with value: 0.888 and parameters: {'n_estimators': 227, 'max_depth': 9, 'min_samples_split': 3, 'max_features': 'log2'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:48,586] Trial 41 finished with value: 0.888 and parameters: {'n_estimators': 63, 'max_depth': 24, 'min_samples_split': 7, 'max_features': 'log2'}. Best is trial 7 with value: 0.9.\n",
      "c:\\Users\\decroux paul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "[I 2024-08-28 17:44:49,155] Trial 42 finished with value: 0.892 and parameters: {'n_estimators': 52, 'max_depth': 28, 'min_samples_split': 5, 'max_features': 'auto'}. Best is trial 7 with value: 0.9.\n",
      "[I 2024-08-28 17:44:49,833] Trial 43 finished with value: 0.908 and parameters: {'n_estimators': 67, 'max_depth': 11, 'min_samples_split': 9, 'max_features': 'log2'}. Best is trial 43 with value: 0.908.\n",
      "[I 2024-08-28 17:44:50,616] Trial 44 finished with value: 0.888 and parameters: {'n_estimators': 95, 'max_depth': 6, 'min_samples_split': 6, 'max_features': 'log2'}. Best is trial 43 with value: 0.908.\n",
      "[I 2024-08-28 17:44:51,289] Trial 45 finished with value: 0.9 and parameters: {'n_estimators': 69, 'max_depth': 12, 'min_samples_split': 20, 'max_features': 'log2'}. Best is trial 43 with value: 0.908.\n",
      "[I 2024-08-28 17:44:51,934] Trial 46 finished with value: 0.892 and parameters: {'n_estimators': 68, 'max_depth': 10, 'min_samples_split': 20, 'max_features': 'log2'}. Best is trial 43 with value: 0.908.\n",
      "[I 2024-08-28 17:44:52,601] Trial 47 finished with value: 0.896 and parameters: {'n_estimators': 79, 'max_depth': 6, 'min_samples_split': 19, 'max_features': 'log2'}. Best is trial 43 with value: 0.908.\n",
      "[I 2024-08-28 17:44:53,701] Trial 48 finished with value: 0.892 and parameters: {'n_estimators': 118, 'max_depth': 12, 'min_samples_split': 19, 'max_features': 'log2'}. Best is trial 43 with value: 0.908.\n",
      "[I 2024-08-28 17:44:54,322] Trial 49 finished with value: 0.896 and parameters: {'n_estimators': 63, 'max_depth': 15, 'min_samples_split': 15, 'max_features': 'log2'}. Best is trial 43 with value: 0.908.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 0.908\n",
      "  Params: \n",
      "    n_estimators: 67\n",
      "    max_depth: 11\n",
      "    min_samples_split: 9\n",
      "    max_features: log2\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Créer des données synthétiques\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=0, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    # Définir les hyperparamètres à optimiser\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2'])\n",
    "\n",
    "    # Créer le modèle RandomForest avec les hyperparamètres proposés\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        max_features=max_features,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Évaluer le modèle\n",
    "    predictions = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Créer un environnement Optuna et commencer l'optimisation\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Afficher les meilleurs hyperparamètres\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have your predictions already\n",
    "# y_pred = model.predict(X_test)\n",
    "y_pred = trial.predict(X_test)  # Example if predictions come from a trial object\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Visualizing the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=trial.classes_)\n",
    "disp.plot()\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])  # Ensure y_pred_proba is your positive class probability\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Display the ROC curve\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Example Model').plot()\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = trial.feature_importances_  # Adjust this line based on your model\n",
    "\n",
    "# Visualizing feature importance\n",
    "plt.barh(range(len(feature_importances)), feature_importances, align='center')\n",
    "plt.yticks(np.arange(len(X_train.columns)), X_train.columns)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Create a SHAP explainer\n",
    "explainer = shap.TreeExplainer(trial)  # Adjust this based on your model type\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Visualize the SHAP values for the first instance\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Génération des données et division\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Application de SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Entraînement du modèle\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Prédiction et évaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Votre fonction objective\n",
    "def objective(trial):\n",
    "    # Chargement de vos données X, y ici\n",
    "    # X, y = load_your_data()\n",
    "\n",
    "    # Séparation des données\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    # Suggestion d'hyperparamètres\n",
    "    C = trial.suggest_loguniform('C', 1e-3, 1e3)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "    gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "    degree = trial.suggest_int('degree', 1, 5) if kernel == 'poly' else 3  # Degré est utilisé seulement pour 'poly'\n",
    "    coef0 = trial.suggest_float('coef0', 0.0, 10.0) if kernel in ['poly', 'sigmoid'] else 0.0\n",
    "\n",
    "    # Création du modèle SVM avec les paramètres suggérés\n",
    "    clf = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree, coef0=coef0, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Prédiction et évaluation en utilisant le rappel\n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    return score\n",
    "\n",
    "# Création et exécution de l'étude\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Affichage des meilleurs hyperparamètres\n",
    "print('Meilleurs hyperparamètres :', study.best_params)\n",
    "print('Meilleur score obtenu :', study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Face aux résultats peu convaincants obtenus avec la Random Forest optimisée, j'ai exploré plusieurs autres approches. Tout d'abord, j'ai tenté de rééquilibrer les données par oversampling, mais bien que cela ait légèrement amélioré les performances, l'impact global est resté marginal. Ensuite, j'ai essayé d'appliquer un smoothing des données, mais là encore, les résultats ont été peu affectés. En dépit de l'utilisation de techniques avancées comme la validation croisée (CV) et l'optimisation avec Optuna, le modèle continue à souffrir d'overfitting important. Cela m'a conduit à analyser la situation, et il semble que la complexité intrinsèque des données et le déséquilibre prononcé entre les classes rendent difficile la généralisation du modèle, même avec ces ajustements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
