{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date deal_id    Portfolio  feature_1  feature_2  feature_3  \\\n",
      "0  2024-08-28  deal_1  Portfolio_1  -0.459183  -0.106103  -0.468682   \n",
      "1  2024-08-28  deal_1  Portfolio_2   0.778182  -1.778243  -0.392431   \n",
      "2  2024-08-28  deal_1  Portfolio_3   1.374551   0.215027   0.080313   \n",
      "3  2024-08-28  deal_2  Portfolio_1  -0.147279   1.201052   1.075295   \n",
      "4  2024-08-28  deal_2  Portfolio_2  -0.804429  -0.031754   0.993130   \n",
      "\n",
      "   binary_feature_1  binary_feature_2  \n",
      "0                 0                 0  \n",
      "1                 0                 0  \n",
      "2                 0                 0  \n",
      "3                 1                 1  \n",
      "4                 0                 0  \n",
      "___________________________________________________________________________\n",
      "                        feature_1_min  feature_1_max  feature_1_mean  \\\n",
      "date       Portfolio                                                   \n",
      "2024-08-28 Portfolio_1      -0.753279       0.896198       -0.085225   \n",
      "           Portfolio_2      -0.804429       1.512625        0.432943   \n",
      "           Portfolio_3      -0.544654       1.374551        0.229446   \n",
      "2024-08-29 Portfolio_1      -0.403277       0.879296        0.214755   \n",
      "           Portfolio_2      -1.434835       0.397962       -0.508106   \n",
      "\n",
      "                        feature_1_var  feature_1_25%  feature_1_75%  \\\n",
      "date       Portfolio                                                  \n",
      "2024-08-28 Portfolio_1       0.392056      -0.459183       0.037421   \n",
      "           Portfolio_2       0.730351       0.129619       0.778182   \n",
      "           Portfolio_3       0.742254      -0.452747       0.906917   \n",
      "2024-08-29 Portfolio_1       0.339257      -0.339648       0.689056   \n",
      "           Portfolio_2       0.689553      -1.016715       0.339488   \n",
      "\n",
      "                        feature_1_sum  feature_2_min  feature_2_max  \\\n",
      "date       Portfolio                                                  \n",
      "2024-08-28 Portfolio_1      -0.426123      -1.032840       1.201052   \n",
      "           Portfolio_2       2.164715      -1.778243       0.881181   \n",
      "           Portfolio_3       1.147229      -0.870110       1.708795   \n",
      "2024-08-29 Portfolio_1       1.073773      -0.904209       0.396756   \n",
      "           Portfolio_2      -2.540529      -0.904383       2.874080   \n",
      "\n",
      "                        feature_2_mean  ...  feature_2_sum  feature_3_min  \\\n",
      "date       Portfolio                    ...                                 \n",
      "2024-08-28 Portfolio_1        0.119177  ...       0.595883      -1.456545   \n",
      "           Portfolio_2       -0.271834  ...      -1.359170      -1.600912   \n",
      "           Portfolio_3        0.598943  ...       2.994716      -0.128372   \n",
      "2024-08-29 Portfolio_1       -0.435204  ...      -2.176020      -1.063327   \n",
      "           Portfolio_2        0.283594  ...       1.417968      -0.229740   \n",
      "\n",
      "                        feature_3_max  feature_3_mean  feature_3_var  \\\n",
      "date       Portfolio                                                   \n",
      "2024-08-28 Portfolio_1       2.715494        0.264605       2.703905   \n",
      "           Portfolio_2       1.660440        0.034948       1.672470   \n",
      "           Portfolio_3       1.284407        0.350343       0.302046   \n",
      "2024-08-29 Portfolio_1       1.389072        0.038205       0.862407   \n",
      "           Portfolio_2       1.996874        0.536040       0.970588   \n",
      "\n",
      "                        feature_3_25%  feature_3_75%  feature_3_sum  \\\n",
      "date       Portfolio                                                  \n",
      "2024-08-28 Portfolio_1      -0.542538       1.075295       1.323024   \n",
      "           Portfolio_2      -0.485486       0.993130       0.174741   \n",
      "           Portfolio_3       0.080313       0.348383       1.751714   \n",
      "2024-08-29 Portfolio_1      -0.406646       0.447738       0.191026   \n",
      "           Portfolio_2      -0.160271       1.118496       2.680201   \n",
      "\n",
      "                        binary_feature_1_sum  binary_feature_2_sum  \n",
      "date       Portfolio                                                \n",
      "2024-08-28 Portfolio_1                     1                     3  \n",
      "           Portfolio_2                     0                     1  \n",
      "           Portfolio_3                     1                     2  \n",
      "2024-08-29 Portfolio_1                     1                     1  \n",
      "           Portfolio_2                     3                     5  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta, date\n",
    "\n",
    "# Function to generate synthetic data\n",
    "def generate_data(num_days, num_deals, num_portfolios):\n",
    "    start_date = date.today()\n",
    "    date_range = [start_date + timedelta(days=x) for x in range(num_days)]\n",
    "\n",
    "    # Generate data\n",
    "    data = []\n",
    "    for current_date in date_range:\n",
    "        for deal_id in range(1, num_deals + 1):\n",
    "            for portfolio in range(1, num_portfolios + 1):\n",
    "                row = {\n",
    "                    'date': current_date,\n",
    "                    'deal_id': f'deal_{deal_id}',\n",
    "                    'Portfolio': f'Portfolio_{portfolio}',\n",
    "                    'feature_1': np.random.randn(),\n",
    "                    'feature_2': np.random.randn(),\n",
    "                    'feature_3': np.random.randn(),\n",
    "                    'binary_feature_1': np.random.choice([0, 1], p=[0.7, 0.3]),\n",
    "                    'binary_feature_2': np.random.choice([0, 1], p=[0.7, 0.3])\n",
    "                }\n",
    "                data.append(row)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Define which columns are binary and which are floating point\n",
    "binary_features = ['binary_feature_1', 'binary_feature_2']\n",
    "float_features = ['feature_1', 'feature_2', 'feature_3']\n",
    "\n",
    "# Function to aggregate data at the portfolio level by date\n",
    "def aggregate_data(df, binary_features, float_features):\n",
    "    agg_dict = {feature: ['min', 'max', 'mean', 'var', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75), 'sum'] for feature in float_features}\n",
    "    agg_dict.update({feature: ['sum'] for feature in binary_features})  # Summing binary features counts the number of 1s\n",
    "\n",
    "    # Rename lambda functions for clarity\n",
    "    for feature in float_features:\n",
    "        agg_dict[feature][4].__name__ = '25%'\n",
    "        agg_dict[feature][5].__name__ = '75%'\n",
    "\n",
    "    # Group by date and Portfolio and apply aggregation\n",
    "    aggregated_df = df.groupby(['date', 'Portfolio']).agg(agg_dict)\n",
    "    aggregated_df.columns = ['_'.join(col).strip() for col in aggregated_df.columns.values]  # Flatten MultiIndex columns\n",
    "\n",
    "    return aggregated_df\n",
    "\n",
    "# Generate synthetic dataset\n",
    "df = generate_data(10, 5, 3)  # 10 days, 5 deals per day, 3 portfolios\n",
    "print(df.head())\n",
    "# Aggregate the data\n",
    "aggregated_df = aggregate_data(df, binary_features, float_features)\n",
    "print('___________________________________________________________________________')\n",
    "# Display the aggregated DataFrame\n",
    "print(aggregated_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SpecificationError",
     "evalue": "nested renamer is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSpecificationError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16228\\1729067533.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m# Example usage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# Assume X_train and X_test are already loaded DataFrames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0maggregated_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maggregate_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[0maggregated_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maggregate_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16228\\1729067533.py\u001b[0m in \u001b[0;36maggregate_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# Apply aggregations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0maggregated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Portfolio'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maggregations_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# Flatten the MultiIndex columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\decroux paul\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGroupByApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 869\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\decroux paul\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36magg\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[1;31m# we require a list, but not a 'str'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\decroux paul\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36magg_dict_like\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    471\u001b[0m             \u001b[0mselection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m         \u001b[0marg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_dictlike_arg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"agg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mselected_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\decroux paul\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mnormalize_dictlike_arg\u001b[1;34m(self, how, obj, func)\u001b[0m\n\u001b[0;32m    582\u001b[0m         ) or (any(is_dict_like(v) for _, v in func.items())):\n\u001b[0;32m    583\u001b[0m             \u001b[1;31m# GH 15931 - deprecation of renaming keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mSpecificationError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nested renamer is not supported\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSpecificationError\u001b[0m: nested renamer is not supported"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def aggregate_features(df):\n",
    "    # Define the columns for which you want to calculate specific aggregations\n",
    "    numerical_cols = ['feature1', 'feature2', 'feature3']  # replace with your actual feature columns\n",
    "    binary_cols = ['binary_feature1', 'binary_feature2']  # replace with your actual binary feature columns\n",
    "    \n",
    "    # Define aggregation functions for numerical columns\n",
    "    aggregations = {\n",
    "        'min': 'min',\n",
    "        'max': 'max',\n",
    "        'mean': 'mean',\n",
    "        'var': 'var',\n",
    "        'q25': lambda x: np.percentile(x, 25),\n",
    "        'q75': lambda x: np.percentile(x, 75),\n",
    "        'iqr': lambda x: np.percentile(x, 75) - np.percentile(x, 25),\n",
    "        'sum': 'sum'\n",
    "    }\n",
    "\n",
    "    # Prepare a dict to hold column-specific aggregations\n",
    "    aggregations_dict = {}\n",
    "    for col in numerical_cols:\n",
    "        aggregations_dict[col] = aggregations\n",
    "    \n",
    "    for col in binary_cols:\n",
    "        aggregations_dict[col] = {'sum': 'sum', 'mean': 'mean'}\n",
    "\n",
    "    # Apply aggregations\n",
    "    aggregated = df.groupby(['date', 'Portfolio']).agg(aggregations_dict)\n",
    "    \n",
    "    # Flatten the MultiIndex columns\n",
    "    aggregated.columns = ['_'.join(col).strip() for col in aggregated.columns.values]\n",
    "\n",
    "    return aggregated\n",
    "\n",
    "# Example usage\n",
    "# Assume X_train and X_test are already loaded DataFrames\n",
    "aggregated_train = aggregate_features(X_train)\n",
    "aggregated_test = aggregate_features(X_test)\n",
    "\n",
    "# Optionally, save or display the aggregated data\n",
    "print(aggregated_train.head())\n",
    "print(aggregated_test.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
